# catenary-activation-function

This library proposes the use of a "catenary" curve as an activation function. Allowing more dimensions of change.


During extant work with mathematics and other machine learning, or I seem to find, that a lot of the mathematics or the best formulae, derive from nature. All the good things or the most important maths that we utilise must bind with a natural philosophy, for example, twist and gravitational forces and all that jazz. It seems to conclude that if artificial intelligence frameworks mimic that of the natural brain, then finding an activation function of which is more intrinsic to natural properties should resolve or should result in a better outcome, right? That's the theory.


Aiming to align artificial neural networks more closely with natural principles. The catenary curve shape could provide a more balanced or smoother transition compared to traditional functions.



The catenary curve in itself is the, "perfect curve". It's the most optimal curve in any given situation for that property or that wire. In turn should work when resolving natural curves in our activation.
All activations are either curvy or very straight, but we have to pick one. What if it were possible to have the perfect curve all the way through the entire function.

Using the catenary curve could indeed offer a more flexible activation function.

The catenary has three additional inputs along with the standard inputs used for an activation.
I call them dimensions.
They are the start and end points and length of the curve itself.

Extending the length would change the depth of the curve. Furthermore, there's obviously a cutoff. Now, in some activations, you can go under this cutoff. In backward propagation, you would have a reverse curve.

However, we can dial this in. An activation function made out a curve that moves, you could have independent neurons managing the start and end positions, and the third neuron managing the length. These could be trained to produce the perfect activation function for that data set.

First and foremost, the activation function could mimic other activation functions by dialing the right settings. You could go from sigmoid to ReLU without changing the activation function itself.


I think that's the real benefit of a catenary. Two different variations of the same curve could elicit different A general appliance is fitting and could serve for more organic nature.
My concept is - if this is a better curve, it should be easier to train and more manage organic activations.  If the catenary curve proves more naturally aligned with neural functions, it could improve both training efficiency and performance.



Additional component to the activation function include the start and end _points_.
If they reside within the k-space of the activation layer, i.e. if the start and end points reside within an area of which can elicit results, rather than starting outside of a 0 and 1 property - The tips of the curve need to be _smoothed_. We need to smooth-off the tips, thereby ensuring that there isn't a fall-off

e.g. they fall between 0 and 1 for the activation space.
I think these two points would be 2D, so you could move them in 2D space, changing the curve, just like you would a normal wire, a real wire, i.e. a chain-linked wire.

Another avenue can be _caching of positions_. A simple equation based upon the width of the fixed floating point numerical decimal.
For example, a floating point 4 only needs 1,000 points. A floating point 16 may need a million points. That would be silly. So you would obviously use a derivative call there.


At the moment Backward propagation isn't completely solved. I'm not sure what type of curve to backpropagate.
My first theory is to take the smooth points of A and B and produce a single hill curve, bump curve if you will, for the reverse.
Other theories would be that you would actually have a true reverse catenary, where you would have an activation function dedicated to backwards propagation, which would be the inverse or the mirror version of the same catenary.

Likely we can derive a backward hump curve given the dimensions of the curve.